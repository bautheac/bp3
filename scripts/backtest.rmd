---
title: "backtest"
author: "Olivier"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: markdown_github
# output: html_document
---
## Setup

R is single-threated by default so I'm setting up a working cluster here to access computational power of all available cores on computer.
```{r setup, message = F, warning = F, include = T}
library(magrittr)

knitr::opts_chunk$set(echo = T, cache = T, comment = "#>")

cluster <- multidplyr::create_cluster(parallel:::detectCores())
```


## Load

I load the data into two dataframes here; one for historical data ("data_price_etf.csv" in the original github repo) and one for qualitative data ("data_static_etf_com.csv" in the original github repo).  
I limit the analysis to 100 randomly picked names here to keep things light as long as developping the code. Will run on the whole dataset when everything is fully developped and works well.

```{r load, message = F, warning = F}
random_names <- names(readr::read_csv(file = "../data/data_historic_etf.csv", n_max = 1L))[-1L]
random_names <- random_names[sample.int(NROW(random_names), 100L)]

historic <- readr::read_csv(file = "../data/data_historic_etf.csv") %>%
  dplyr::mutate(date = as.Date(Date, origin = "1970-01-01")) %>% dplyr::arrange(date) %>%
  dplyr::select(-Date) %>% tidyr::gather(`name`, price, -date) %>% 
  dplyr::mutate(price = as.numeric(price)) %>% dplyr::filter(complete.cases(.)) %>%
  dplyr::filter(name %in% random_names)
static <- readr::read_csv(file = "../data/data_static_etf.csv") %>%
  dplyr::rename(name = ticker) %>% tidyr::gather(field, value, -name) %>%
  dplyr::mutate(field = forcats::as_factor(gsub(x = field, pattern = "_", replacement = "."))) %>%
  tidyr::spread(field, value) %>%
  dplyr::filter(name %in% random_names)
horizons <- c(4L, 8L, 13L, 26L, 52L)
multidplyr::cluster_assign_value(cluster, "horizons", horizons)
```


## Transform

To my understanding, feature engineering here involves calculating statistics as well as constructing TDA features at regular interval (monthly) from the time series data at regular interval (monthly). The resulting dataframe would contain, for each month, one sample per name (ETF) where features include time series statistics, TDA values as well as qualitative information (static dataframe).

### Historic features

I understand that time series statistics include return, high minus low as well as volatility over various time horizons including past week, past 4 weeks, past 8 weeks, past 13 weeks, past 26 weeks, past 52 weeks.

#### Returns

I calculate returns as the relative change in price over the above-mentioned time horizons here.
```{r returns, message = F, warning = F}
returns <- historic %>% dplyr::group_by(`name`) %>%
  dplyr::do({data <- .; data.table::rbindlist(
    lapply(c(1L, horizons), function(x)
      tibble::tibble(value = data$price / dplyr::lag(data$price, n = x) - 1L,
                     date = data$date, 
                     field = paste("return", x, ifelse(x > 1L, "weeks", "week"), sep = "."))
    ))}) %>% dplyr::select(name, field, date, value) %>% dplyr::filter(! is.na(value)) %>% 
  dplyr::mutate(field = forcats::as_factor(field)) %>% tidyr::spread(field, value)
```

#### High - low

I calculate high minus low by substracting the minimum observed value from the maximum observed value over the above-mentioned time horizons here.
```{r `high - low`, message = F, warning = F}
HmL <- function(x) max(x) - min(x)
multidplyr::cluster_assign_value(cluster, "HmL", HmL)

HmL <- historic %>% multidplyr::partition(name, cluster = cluster) %>%
  dplyr::do({data <- .; data.table::rbindlist(
    lapply(horizons, function(x)
      tibble::tibble(value = tryCatch(tibbletime::rollify(HmL, window = x)(data$price),
                                      error = function(e) NA),
                     date = data$date, 
                     field = paste("HmL", x, "weeks", sep = "."))
    ))}) %>% dplyr::collect() %>% dplyr::select(name, field, date, value) %>%
  dplyr::filter(! is.na(value)) %>% dplyr::mutate(field = forcats::as_factor(field)) %>%
  tidyr::spread(field, value)
```

#### Volatility

I calculate volatility as the annualized standard deviation of 1-week returns over the above-mentioned time horizons here.
```{r volatility, message = F, warning = F}
volatility <- function(x) sd(x, na.rm = T) * sqrt(52L)
multidplyr::cluster_assign_value(cluster, "volatility", volatility)

volatility <- dplyr::select(returns, name, date, return = return.1.week) %>%
  multidplyr::partition(name, cluster = cluster) %>%
  dplyr::do({data <- .; data.table::rbindlist(
    lapply(horizons, function(x)
      tibble::tibble(value = tryCatch(tibbletime::rollify(volatility, window = x)(data$return),
                                      error = function(e) NA),
                     date = data$date, 
                     field = paste("volatility", x, "weeks", sep = "."))
    ))}) %>% dplyr::collect() %>% dplyr::select(name, field, date, value) %>%
  dplyr::filter(! is.na(value)) %>% dplyr::mutate(field = forcats::as_factor(field)) %>% 
  tidyr::spread(field, value)
```

#### TDA
Ok, got it, see bellow if things look alright to you.
```{r TDA, message = F, warning = F}
TDA <- function(x) {
  diag <- TDA::gridDiag(FUNvalues = x ,sublevel = F, printProgress = F)$diagram
  values <- seq(min(diag[, c("Death", "Birth")]), max(diag[, c("Death", "Birth")]), 
                length = 50L)
  out <- c(TDA::landscape(diag, dimension = 0L, KK = 2L, tseq = values)[, 1L],
           TDA::landscape(diag, dimension = 0L, KK = 3L, tseq = values)[, 1L])
  
  magrittr::set_names(as.list(out), paste("TDA", 1L:100L))
}
multidplyr::cluster_assign_value(cluster, "TDA", TDA)

TDA <- historic %>% multidplyr::partition(name, cluster = cluster) %>%
  dplyr::do({ data <- .; 
  tryCatch(
    data.table::rbindlist(
      lapply(52L:nrow(data), function(x)
        cbind(date = dplyr::filter(data, dplyr::row_number() == x)$date,
              data.frame(TDA(dplyr::slice(data, (x - 51L):x)$price))))
    ),
    error = function(e) 
      data.frame(magrittr::set_names(as.list(rep(NA, 101L)), c("date", paste("TDA", 1L:100L))))
    )
  }) %>% dplyr::collect()
```

#### Wrap up
```{r `historic final`, message = F, warning = F}
data <- Reduce(function(x, y) merge(x, y, by = c("name", "date"), all = T),
               list(returns, HmL, volatility, TDA)) %>%
  dplyr::mutate(year = lubridate::year(date), month = lubridate::month(date)) %>%
  dplyr::group_by(name, year, month) %>% dplyr::filter(dplyr::row_number() == n()) %>% 
  dplyr::group_by(name) %>% dplyr::slice(52L:n()) %>% dplyr::ungroup() %>%
  dplyr::select(-c("year", "month"))
```

### Merge with qualitative features dataset

And there is the features dataset; does everything looks alright to you?
```{r `merge static`, message = F, warning = F}
data %<>% dplyr::left_join(static, by = "name")
head(data)
```
```{r}
names(data)
```


Moving on to working on modeling part.
